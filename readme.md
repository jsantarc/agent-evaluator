# Agent Evaluator

A minimal Gradio interface for testing and validating chatbot agents across different stages.

## ğŸ¯ Purpose

This repository provides a lightweight framework for chatbot validation testing. Built with minimal code to focus on agent evaluation rather than complex infrastructure.

## ğŸš€ Features

- **Minimal Codebase**: Clean, simple implementation for quick setup and testing
- **Gradio Interface**: User-friendly chat interface for interactive agent evaluation
- **LangChain Integration**: Conversation memory and chain management
- **OpenAI Integration**: Tests agents powered by GPT models
- **Docker Support**: Containerized for consistent deployment

## ğŸ“‹ Requirements

- Python 3.11+
- OpenAI API key
- Docker (for containerized deployment)

## ğŸ› ï¸ Local Setup

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd agent-evaluator
   ```

2. **Create `.env` file**
   ```bash
   echo "OPENAI_API_KEY=your-key-here" > .env
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   python app.py
   ```

5. **Access the interface**
   - Open browser to `http://localhost:7860`

## ğŸ³ Docker Deployment

1. **Build the image**
   ```bash
   docker build -t agent-evaluator .
   ```

2. **Run the container**
   ```bash
   docker run -p 7860:7860 -e OPENAI_API_KEY=your-key-here agent-evaluator
   ```

## ğŸ“ Project Structure

```
agent-evaluator/
â”œâ”€â”€ app.py              # Main Gradio chatbot interface
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Dockerfile         # Container configuration
â”œâ”€â”€ .env              # Environment variables (not in git)
â””â”€â”€ README.md         # This file
```

## ğŸ” Security

- **Never commit your `.env` file** - it contains your API keys
- Use environment variables for production deployments
- The `.gitignore` file protects sensitive data

## ğŸ§ª Testing Validation Stages

This minimal setup allows you to:
- Test conversational memory
- Validate response quality
- Evaluate agent behavior across multiple turns
- Compare different model configurations

## ğŸ“ Technologies

- **Gradio**: Web interface framework
- **LangChain**: LLM orchestration and memory
- **OpenAI**: Language model provider
- **Docker**: Containerization

## ğŸ¤ Contributing

This is a minimal evaluation framework. Feel free to fork and extend for your specific validation needs.

## ğŸ“„ License

MIT License - feel free to use for your chatbot evaluation projects.

---

**Built for simplicity. Focused on validation.**